{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure Credentials\n",
    "subscription_key = os.getenv(\"AZURE_SUBSCRIPTION_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_VISION_ENDPOINT\")\n",
    "\n",
    "# OpenAI Credentials\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key= os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_name = 'gpt-4o'\n",
    "api_version = '2023-12-01-preview' \n",
    "\n",
    "# Pre-requisites (uncomment to pip install if not using the venv. to setup the venv, see readme)\n",
    "# !pip install azure-cognitiveservices-vision-computervision matplotlib pillow openai\n",
    "\n",
    "# use a .ttf font that can support all languages you want to translate to\n",
    "# e.g. use https://github.com/notofonts/noto-cjk/tree/main/Sans\n",
    "FONT_PATH = \"./fonts/NotoSans-Medium.ttf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with ComputerVisionClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputerVisionOcrErrorException",
     "evalue": "Operation returned an invalid status code 'PermissionDenied'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mComputerVisionOcrErrorException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/images/bicycle.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m image_stream:\n\u001b[1;32m---> 23\u001b[0m     ocr_result \u001b[38;5;241m=\u001b[39m \u001b[43mcomputervision_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_in_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m operation_location \u001b[38;5;241m=\u001b[39m ocr_result\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation-Location\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation Location: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sms79\\dev\\microsoft_translation_public\\.venv\\lib\\site-packages\\azure\\cognitiveservices\\vision\\computervision\\operations\\_computer_vision_client_operations.py:1709\u001b[0m, in \u001b[0;36mComputerVisionClientOperationsMixin.read_in_stream\u001b[1;34m(self, image, language, pages, model_version, reading_order, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[0;32m   1706\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moperation_config)\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m202\u001b[39m]:\n\u001b[1;32m-> 1709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mComputerVisionOcrErrorException(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize, response)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw:\n\u001b[0;32m   1712\u001b[0m     client_raw_response \u001b[38;5;241m=\u001b[39m ClientRawResponse(\u001b[38;5;28;01mNone\u001b[39;00m, response)\n",
      "\u001b[1;31mComputerVisionOcrErrorException\u001b[0m: Operation returned an invalid status code 'PermissionDenied'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Computer Vision Client\n",
    "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
    "\n",
    "\n",
    "image_path = \"./data/images/bicycle.png\"\n",
    "\n",
    "with open(image_path, \"rb\") as image_stream:\n",
    "    ocr_result = computervision_client.read_in_stream(image_stream, raw=True)\n",
    "\n",
    "operation_location = ocr_result.headers[\"Operation-Location\"]\n",
    "print(f\"Operation Location: {operation_location}\")\n",
    "\n",
    "operation_id = operation_location.split(\"/\")[-1]\n",
    "print(f\"Operation ID: {operation_id}\")\n",
    "\n",
    "while True:\n",
    "    result = computervision_client.get_read_result(operation_id)\n",
    "    print(f\"Status: {result.status}\")\n",
    "    if result.status not in ['notStarted', 'running']:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "    read_results = result.analyze_result.read_results\n",
    "    line_bounding_boxes = []\n",
    "    if read_results:\n",
    "        for read_result in read_results:\n",
    "            for line in read_result.lines:\n",
    "                line_bounding_boxes.append({\n",
    "                    \"text\": line.text,\n",
    "                    \"bounding_box\": line.bounding_box,\n",
    "                    \"confidence\": line.appearance.style.confidence if line.appearance else None\n",
    "                })\n",
    "        line_bounding_boxes\n",
    "    else:\n",
    "        raise Exception(\"No text was recognized in the image.\")\n",
    "else:\n",
    "    raise Exception(\"OCR operation did not succeed.\")\n",
    "\n",
    "image = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "font_size = 20\n",
    "font = ImageFont.truetype(FONT_PATH, font_size)\n",
    "    \n",
    "\n",
    "for line_info in line_bounding_boxes:\n",
    "    print(line_info)\n",
    "    bounding_box = line_info['bounding_box']\n",
    "    confidence = line_info['confidence']\n",
    "    pts = [(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)]\n",
    "    \n",
    "\n",
    "    # Draw thicker polygon for bounding box with width parameter\n",
    "    draw.line(pts + [pts[0]], fill=\"yellow\", width=4)\n",
    "    \n",
    "    # Coordinates for the text\n",
    "    x, y = bounding_box[0], bounding_box[1] - font_size\n",
    "\n",
    "    # Draw white text outline\n",
    "    outline_range = 2\n",
    "    for dx in range(-outline_range, outline_range + 1):\n",
    "        for dy in range(-outline_range, outline_range + 1):\n",
    "            if dx != 0 or dy != 0:\n",
    "                draw.text((x + dx, y + dy), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"white\")\n",
    "\n",
    "    # Draw black text\n",
    "    draw.text((x, y), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"black\")    \n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(\"Image with Bounding Boxes\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "original_image = Image.open(image_path)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.array(original_image))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how ComputerVision client works, but the the detection seems to be quite fragmented, and line detection is not good. Let's try the other alternative, ImageAnalysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageAnalysisClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Image Analysis Client\n",
    "image_analysis_client = ImageAnalysisClient(endpoint, AzureKeyCredential(subscription_key))\n",
    "\n",
    "image_path = \"./data/images/bicycle.png\"\n",
    "\n",
    "with open(image_path, \"rb\") as image_stream:\n",
    "    image_data = image_stream.read()\n",
    "    result = image_analysis_client.analyze(\n",
    "        image_data=image_data,\n",
    "        visual_features=[VisualFeatures.READ],\n",
    "    )\n",
    "\n",
    "if result.read is not None:\n",
    "    line_bounding_boxes = []\n",
    "    for line in result.read.blocks[0].lines:\n",
    "        bounding_box = []\n",
    "        for point in line.bounding_polygon:\n",
    "            bounding_box.append(point.x)\n",
    "            bounding_box.append(point.y)\n",
    "        line_bounding_boxes.append({\n",
    "            \"text\": line.text,\n",
    "            \"bounding_box\": bounding_box,\n",
    "            \"confidence\": line.words[0].confidence if line.words else None\n",
    "        })\n",
    "else:\n",
    "    raise Exception(\"No text was recognized in the image.\")\n",
    "\n",
    "\n",
    "image = Image.open(image_path)\n",
    "draw = ImageDraw.Draw(image)\n",
    "font_size = 20\n",
    "font = ImageFont.truetype(FONT_PATH, font_size)\n",
    "    \n",
    "\n",
    "for line_info in line_bounding_boxes:\n",
    "    print(line_info)\n",
    "    bounding_box = line_info['bounding_box']\n",
    "    confidence = line_info['confidence']\n",
    "    pts = [(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)]\n",
    "    \n",
    "\n",
    "    # Draw thicker polygon for bounding box with width parameter\n",
    "    draw.line(pts + [pts[0]], fill=\"yellow\", width=4)\n",
    "    \n",
    "    # Coordinates for the text\n",
    "    x, y = bounding_box[0], bounding_box[1] - font_size\n",
    "\n",
    "    # Draw white text outline\n",
    "    outline_range = 2\n",
    "    for dx in range(-outline_range, outline_range + 1):\n",
    "        for dy in range(-outline_range, outline_range + 1):\n",
    "            if dx != 0 or dy != 0:\n",
    "                draw.text((x + dx, y + dy), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"white\")\n",
    "\n",
    "    # Draw black text\n",
    "    draw.text((x, y), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"black\")    \n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.array(image))\n",
    "plt.title(\"Image with Bounding Boxes\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "original_image = Image.open(image_path)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.array(original_image))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see improvement in line detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions for Image Analysis via Azure's API\n",
    "- We define functions to use the image analysis API\n",
    "- We also save bounding boxes and text data to a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Image Analysis Client\n",
    "image_analysis_client = ImageAnalysisClient(endpoint, AzureKeyCredential(subscription_key))\n",
    "\n",
    "# Function to Get Line Bounding Boxes\n",
    "def get_line_bounding_boxes(image_path):\n",
    "    with open(image_path, \"rb\") as image_stream:\n",
    "        image_data = image_stream.read()\n",
    "        result = image_analysis_client.analyze(\n",
    "            image_data=image_data,\n",
    "            visual_features=[VisualFeatures.READ],\n",
    "        )\n",
    "\n",
    "    if result.read is not None:\n",
    "        line_bounding_boxes = []\n",
    "        for line in result.read.blocks[0].lines:\n",
    "            bounding_box = []\n",
    "            for point in line.bounding_polygon:\n",
    "                bounding_box.append(point.x)\n",
    "                bounding_box.append(point.y)\n",
    "            line_bounding_boxes.append({\n",
    "                \"text\": line.text,\n",
    "                \"bounding_box\": bounding_box,\n",
    "                \"confidence\": line.words[0].confidence if line.words else None\n",
    "            })\n",
    "        return line_bounding_boxes\n",
    "    else:\n",
    "        raise Exception(\"No text was recognized in the image.\")\n",
    "\n",
    "# Function to Save Bounding Boxes and Confidence Scores as JSON\n",
    "def save_bounding_boxes(image_path, bounding_boxes):\n",
    "    base_name = os.path.basename(image_path)\n",
    "    name, _ = os.path.splitext(base_name)\n",
    "    output_dir = \"./data/bounding_boxes\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"{name}.json\")\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(bounding_boxes, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Function to Load Bounding Boxes and Confidence Scores from JSON\n",
    "def load_bounding_boxes(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "# Function to Plot Bounding Boxes on Image. Set display=True to display the image in a notebook.\n",
    "# Saves images to ./analyzed_images\n",
    "def plot_bounding_boxes(image_path, line_bounding_boxes, display=True):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs('./analyzed_images', exist_ok=True)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    font_size = 20\n",
    "\n",
    "    font = ImageFont.truetype(FONT_PATH, font_size)\n",
    "    \n",
    "    for line_info in line_bounding_boxes:\n",
    "        print(line_info)\n",
    "        bounding_box = line_info['bounding_box']\n",
    "        confidence = line_info['confidence']\n",
    "        pts = [(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)]\n",
    "        \n",
    "        # Draw thicker polygon for bounding box with width parameter\n",
    "        draw.line(pts + [pts[0]], fill=\"yellow\", width=4)\n",
    "        \n",
    "        # Coordinates for the text\n",
    "        x, y = bounding_box[0], bounding_box[1] - font_size\n",
    "\n",
    "        # Draw white text outline\n",
    "        outline_range = 2\n",
    "        for dx in range(-outline_range, outline_range + 1):\n",
    "            for dy in range(-outline_range, outline_range + 1):\n",
    "                if dx != 0 or dy != 0:\n",
    "                    draw.text((x + dx, y + dy), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"white\")\n",
    "\n",
    "        # Draw black text\n",
    "        draw.text((x, y), f\"{line_info['text']} ({confidence:.2f})\", font=font, fill=\"black\")\n",
    "    \n",
    "    # Save the annotated image\n",
    "    output_path = os.path.join('./data/analyzed_images', os.path.basename(image_path))\n",
    "    image.save(output_path)\n",
    "    \n",
    "    if display:\n",
    "        # Display the image\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(np.array(image))\n",
    "        plt.title(\"Image with Bounding Boxes\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        original_image = Image.open(image_path)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(np.array(original_image))\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# High-level function to process an image given a list of image paths\n",
    "def process_image_paths(image_paths):\n",
    "    output_dir = \"./bounding_boxes\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        if image_path.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            print(f\"Processing {image_path}\")\n",
    "            line_bounding_boxes = get_line_bounding_boxes(image_path)\n",
    "            if line_bounding_boxes:\n",
    "                save_bounding_boxes(image_path, line_bounding_boxes)\n",
    "                plot_bounding_boxes(image_path, line_bounding_boxes, display=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning Detected Text into Translation Prompts\n",
    "- Now that we have successfully extracted the data from the image, we need it to be translated.\n",
    "- We thus generate a prompt that contains the translated text line-by-line.\n",
    "- We ask a LLM to translate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bounding_boxes_by_image_path(image_path):\n",
    "    image_name = os.path.basename(image_path).split(\".\")[0]\n",
    "    json_path = f\"./data/bounding_boxes/{image_name}.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        bounding_boxes = load_bounding_boxes(json_path)\n",
    "        if os.path.exists(image_path):\n",
    "            return bounding_boxes\n",
    "        else:\n",
    "            print(f\"Image file {image_path} does not exist.\")\n",
    "    else:\n",
    "        print(f\"Bounding box data {json_path} does not exist.\")\n",
    "\n",
    "def extract_text_from_image_path(image_path):\n",
    "    image_name = os.path.basename(image_path).split(\".\")[0]\n",
    "    json_path = f\"./bounding_boxes/{image_name}.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        bounding_boxes = load_bounding_boxes(json_path)\n",
    "        if not os.path.exists(image_path):\n",
    "            raise Exception(f\"Image file {image_path} does not exist.\")\n",
    "    else:\n",
    "        print(f\"Bounding box data {json_path} does not exist. Generating...\" )\n",
    "        bounding_boxes = get_line_bounding_boxes(image_path)\n",
    "        if bounding_boxes:\n",
    "            save_bounding_boxes(image_path, bounding_boxes)\n",
    "        else:\n",
    "            raise Exception(\"No text was recognized in the image.\")\n",
    "\n",
    "    data = list()\n",
    "    for bounding_box in bounding_boxes:\n",
    "        data.append((bounding_box[\"text\"]))\n",
    "        \n",
    "\n",
    "    return data\n",
    "\n",
    "def gen_image_translation_prompt(image_path, language):\n",
    "    text_data = extract_text_from_image_path(image_path)\n",
    "    prompt =\\\n",
    "f'''\n",
    "You are a translator that receives a batch of lines in an image . Given the following yaml file, please translate each line into {language}. \n",
    "For each line, fill it in with the translation, respecting the context of the text.\n",
    "Return only the yaml file, fully filled in.\n",
    "'''\n",
    "    \n",
    "    for line in text_data:\n",
    "        prompt += f\"- {line}\\n\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./data/images/bicycle.png\"                \n",
    "process_image_paths([image_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_data = retrieve_bounding_boxes_by_image_path(\"./data/images/bicycle.png\")\n",
    "print(bbox_data)\n",
    "print()\n",
    "\n",
    "print(extract_text_from_image_path(\"./data/images/bicycle.png\"))\n",
    "print()\n",
    "\n",
    "print(gen_image_translation_prompt(\"./data/images/bicycle.png\", \"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying OpenAI Translation\n",
    "- GPT4o is used to translate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import re\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,  \n",
    "    api_version=api_version,\n",
    "    base_url=f\"{api_base}/openai/deployments/{deployment_name}\"\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": gen_image_translation_prompt(\"./data/images/bicycle.png\", \"spanish\")\n",
    "            },\n",
    "        ] } \n",
    "    ],\n",
    "    max_tokens=2000 \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4o respects the output notation– we only get a code block with the translation. We then have to remove the markdown code block, and extract out the yaml lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_code_backticks(message):\n",
    "    match = re.match(r'```(?:\\w+)?\\n(.*?)\\n```', message, re.DOTALL)\n",
    "    return match.group(1) if match else message\n",
    "\n",
    "def extract_yaml_lines(message):\n",
    "    lines = message.split('\\n')\n",
    "    yaml_lines = [line[2:] for line in lines if line.startswith('- ')]\n",
    "    return yaml_lines\n",
    "\n",
    "def get_translated_text_data(image_path, language):\n",
    "    prompt = gen_image_translation_prompt(image_path, language)\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "            { \"role\": \"user\", \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": prompt\n",
    "                },\n",
    "            ] } \n",
    "        ],\n",
    "        max_tokens=2000 \n",
    "    )\n",
    "    res = extract_yaml_lines(remove_code_backticks(response.choices[0].message.content))\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {res}\")\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_code_backticks(response.choices[0].message.content))\n",
    "print(extract_yaml_lines(remove_code_backticks(response.choices[0].message.content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing Translated Text\n",
    "- We draw the translated text on the image\n",
    "- We warp it to fit the bounding boxes of the original text\n",
    "- We also set the colour to follow the average colour of the bounding box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageStat\n",
    "\n",
    "\n",
    "# Function to get the average color of a bounding box area\n",
    "def get_average_color(image, bounding_box):\n",
    "    mask = Image.new(\"L\", image.size, 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    pts = [(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)]\n",
    "    draw.polygon(pts, fill=255)\n",
    "    stat = ImageStat.Stat(image, mask)\n",
    "    avg_color = tuple(int(x) for x in stat.mean[:3])  # Ensure it's a tuple of ints\n",
    "    return avg_color\n",
    "\n",
    "# Function to determine the grayscale color for text\n",
    "def get_text_color(bg_color):\n",
    "    # Using luminance formula to determine if the text should be black or white\n",
    "    luminance = (0.299*bg_color[0] + 0.587*bg_color[1] + 0.114*bg_color[2])/255\n",
    "    return (0, 0, 0) if luminance > 0.5 else (255, 255, 255)\n",
    "\n",
    "# Function to apply perspective warp to text image\n",
    "def warp_image_to_bounding_box(image, bounding_box, image_width, image_height):\n",
    "    h, w = image.shape[:2]\n",
    "    src_pts = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n",
    "    dst_pts = np.float32([(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)])\n",
    "    matrix = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "    warped = cv2.warpPerspective(image, matrix, (image_width, image_height))\n",
    "    return warped\n",
    "\n",
    "# Function to draw text onto an image\n",
    "def draw_text_on_image(text, font, text_color):\n",
    "    # Create an image with transparent background\n",
    "    size = font.getbbox(text)[2:]  # width and height of the text\n",
    "    text_image = Image.new('RGBA', size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(text_image)\n",
    "    draw.text((0, 0), text, font=font, fill=text_color)\n",
    "    return text_image\n",
    "\n",
    "# Function to create a filled polygon mask\n",
    "def create_filled_polygon_mask(bounding_box, image_size, fill_color):\n",
    "    mask_image = Image.new('RGBA', image_size, (255, 255, 255, 0))\n",
    "    mask_draw = ImageDraw.Draw(mask_image)\n",
    "    pts = [(bounding_box[i], bounding_box[i+1]) for i in range(0, len(bounding_box), 2)]\n",
    "    mask_draw.polygon(pts, fill=fill_color)\n",
    "    return mask_image\n",
    "\n",
    "# Function to Plot Annotated Image\n",
    "def plot_annotated_image(image_path, line_bounding_boxes, translated_text_list):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs('./data/translated_images', exist_ok=True)\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGBA')\n",
    "    \n",
    "    font_size = 40\n",
    "    font = ImageFont.truetype(FONT_PATH, font_size)\n",
    "    \n",
    "    for line_info, translated_text in zip(line_bounding_boxes, translated_text_list):\n",
    "        bounding_box = line_info['bounding_box']\n",
    "\n",
    "        # Get the average color of the bounding box area\n",
    "        bg_color = get_average_color(image, bounding_box)\n",
    "        text_color = get_text_color(bg_color)\n",
    "\n",
    "        # Create a mask to fill the bounding box area with the background color\n",
    "        mask_image = create_filled_polygon_mask(bounding_box, image.size, bg_color)\n",
    "        \n",
    "        # Composite the mask onto the image to fill the bounding box\n",
    "        image = Image.alpha_composite(image, mask_image)\n",
    "        \n",
    "        # Draw the translated text onto a temporary image\n",
    "        text_image = draw_text_on_image(translated_text, font, text_color)\n",
    "        \n",
    "        # Convert the text image to an array and warp it to fit the bounding box\n",
    "        text_image_array = np.array(text_image)\n",
    "        warped_text_image = warp_image_to_bounding_box(text_image_array, bounding_box, image.width, image.height)\n",
    "        \n",
    "        # Convert the warped text image back to PIL format and paste it onto the original image\n",
    "        warped_text_image_pil = Image.fromarray(warped_text_image)\n",
    "        image = Image.alpha_composite(image, warped_text_image_pil)\n",
    "    \n",
    "    # Save the annotated image\n",
    "    output_path = os.path.join('./data/translated_images', os.path.basename(image_path))\n",
    "    image.save(output_path)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image.convert('RGB'))\n",
    "    plt.title(\"Annotated Image with Translated Text\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    original_image = Image.open(image_path)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "imagepath = \"./data/images/bicycle.png\"\n",
    "plot_annotated_image(imagepath, retrieve_bounding_boxes_by_image_path(imagepath), get_translated_text_data(imagepath, \"Spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Level Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_image(image_path, language):\n",
    "    plot_annotated_image(image_path, retrieve_bounding_boxes_by_image_path(image_path), get_translated_text_data(image_path, language))\n",
    "    \n",
    "translate_image(\"./data/images/korean.png\", \"English\")\n",
    "translate_image(\"./data/images/microsoft1.png\", \"Malay\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
