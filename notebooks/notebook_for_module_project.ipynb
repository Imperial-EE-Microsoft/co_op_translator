{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Azure Credentials\n",
    "subscription_key = os.getenv(\"AZURE_SUBSCRIPTION_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_VISION_ENDPOINT\")\n",
    "\n",
    "# OpenAI Credentials\n",
    "api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_name = 'gpt-4o'\n",
    "api_version = '2023-12-01-preview'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries from the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from the project\n",
    "from src.image_translator.azure_vision_translator import extract_line_bounding_boxes, extract_text_from_image\n",
    "from src.text_translator.openai_translator import translate_text\n",
    "from src.utils.image_utils import plot_annotated_image, save_bounding_boxes, load_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# Path to font file\n",
    "FONT_PATH = \"./fonts/NotoSans-Medium.ttf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Computer Vision Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bounding_boxes_by_image_path(image_path):\n",
    "    \"\"\"\n",
    "    Retrieve bounding boxes for a given image path by loading from a saved JSON file.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of bounding boxes and text data.\n",
    "    \"\"\"\n",
    "    image_name = os.path.basename(image_path).split(\".\")[0]\n",
    "    json_path = f\"./bounding_boxes/{image_name}.json\"\n",
    "    if os.path.exists(json_path):\n",
    "        return load_bounding_boxes(json_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Bounding box data not found for image: {image_path}\")\n",
    "\n",
    "# Function to process the image and annotate with bounding boxes and translated text\n",
    "def process_and_translate_image(image_path, target_language):\n",
    "    # Extract text and bounding boxes\n",
    "    line_bounding_boxes = extract_line_bounding_boxes(image_path)\n",
    "\n",
    "    # Display and save bounding boxes\n",
    "    if line_bounding_boxes:\n",
    "        for line_info in line_bounding_boxes:\n",
    "            print(line_info)\n",
    "        save_bounding_boxes(image_path, line_bounding_boxes)\n",
    "    else:\n",
    "        raise Exception(\"No text was recognized in the image.\")\n",
    "\n",
    "    # Generate translation prompt and get translated text\n",
    "    text_data = [line['text'] for line in line_bounding_boxes]\n",
    "    translated_text_data = translate_text(text_data, target_language)\n",
    "\n",
    "    # Annotate image with translated text\n",
    "    plot_annotated_image(image_path, line_bounding_boxes, translated_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level function to translate image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputerVisionOcrErrorException",
     "evalue": "Operation returned an invalid status code 'PermissionDenied'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mComputerVisionOcrErrorException\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      9\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/bicycle.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mprocess_and_translate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSpanish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m translate_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/korean.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m translate_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/images/microsoft1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMalay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mprocess_and_translate_image\u001b[1;34m(image_path, target_language)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_and_translate_image\u001b[39m(image_path, target_language):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract text and bounding boxes\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     line_bounding_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mextract_line_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Display and save bounding boxes\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line_bounding_boxes:\n",
      "File \u001b[1;32mc:\\users\\sms79\\dev\\microsoft_translation_public\\src\\image_translator\\azure_vision_translator.py:52\u001b[0m, in \u001b[0;36mextract_line_bounding_boxes\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m client \u001b[38;5;241m=\u001b[39m get_computervision_client()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m image_stream:\n\u001b[1;32m---> 52\u001b[0m     ocr_result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_in_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m operation_location \u001b[38;5;241m=\u001b[39m ocr_result\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation-Location\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     54\u001b[0m operation_id \u001b[38;5;241m=\u001b[39m operation_location\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\sms79\\dev\\microsoft_translation_public\\.venv\\lib\\site-packages\\azure\\cognitiveservices\\vision\\computervision\\operations\\_computer_vision_client_operations.py:1709\u001b[0m, in \u001b[0;36mComputerVisionClientOperationsMixin.read_in_stream\u001b[1;34m(self, image, language, pages, model_version, reading_order, custom_headers, raw, callback, **operation_config)\u001b[0m\n\u001b[0;32m   1706\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moperation_config)\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m202\u001b[39m]:\n\u001b[1;32m-> 1709\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m models\u001b[38;5;241m.\u001b[39mComputerVisionOcrErrorException(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize, response)\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw:\n\u001b[0;32m   1712\u001b[0m     client_raw_response \u001b[38;5;241m=\u001b[39m ClientRawResponse(\u001b[38;5;28;01mNone\u001b[39;00m, response)\n",
      "\u001b[1;31mComputerVisionOcrErrorException\u001b[0m: Operation returned an invalid status code 'PermissionDenied'"
     ]
    }
   ],
   "source": [
    "# High-level function to translate image\n",
    "def translate_image(image_path, language):\n",
    "    line_bounding_boxes = retrieve_bounding_boxes_by_image_path(image_path)\n",
    "    text_data = [line['text'] for line in line_bounding_boxes]\n",
    "    translated_text_data = translate_text(text_data, language)\n",
    "    plot_annotated_image(image_path, line_bounding_boxes, translated_text_data)\n",
    "\n",
    "# Example usage\n",
    "image_path = \"../data/images/bicycle.png\"\n",
    "process_and_translate_image(image_path, \"Spanish\")\n",
    "\n",
    "translate_image(\"../data/images/korean.png\", \"English\")\n",
    "translate_image(\"../data/images/microsoft1.png\", \"Malay\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
